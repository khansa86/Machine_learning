{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb 18 09:55:25 2020\n",
    "\n",
    "@author: hinaa\n",
    "\"\"\"\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense,Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16095 images belonging to 3 classes.\n",
      "Found 3924 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\conda\\lib\\site-packages\\ipykernel_launcher.py:62: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "E:\\conda\\lib\\site-packages\\ipykernel_launcher.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=<keras.pre..., steps_per_epoch=178, epochs=20, validation_steps=3924)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "178/178 [==============================] - 571s 3s/step - loss: 1.1158 - accuracy: 0.4487 - val_loss: 1.1190 - val_accuracy: 0.3061\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 506s 3s/step - loss: 0.9387 - accuracy: 0.5426 - val_loss: 1.0863 - val_accuracy: 0.3891\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 560s 3s/step - loss: 0.8457 - accuracy: 0.5989 - val_loss: 0.7892 - val_accuracy: 0.6404\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 591s 3s/step - loss: 0.7860 - accuracy: 0.6392 - val_loss: 0.7153 - val_accuracy: 0.6696\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 645s 4s/step - loss: 0.7381 - accuracy: 0.6648 - val_loss: 0.6450 - val_accuracy: 0.6932\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 610s 3s/step - loss: 0.7143 - accuracy: 0.6812 - val_loss: 0.5183 - val_accuracy: 0.7144\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 571s 3s/step - loss: 0.6830 - accuracy: 0.6949 - val_loss: 0.6557 - val_accuracy: 0.7234\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 586s 3s/step - loss: 0.6582 - accuracy: 0.7069 - val_loss: 0.6338 - val_accuracy: 0.7179\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 573s 3s/step - loss: 0.6458 - accuracy: 0.7183 - val_loss: 0.7070 - val_accuracy: 0.6763\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 585s 3s/step - loss: 0.6311 - accuracy: 0.7213 - val_loss: 0.6139 - val_accuracy: 0.7424\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 570s 3s/step - loss: 0.6289 - accuracy: 0.7272 - val_loss: 0.4651 - val_accuracy: 0.7479\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 583s 3s/step - loss: 0.6083 - accuracy: 0.7312 - val_loss: 0.6037 - val_accuracy: 0.7332\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 567s 3s/step - loss: 0.5969 - accuracy: 0.7407 - val_loss: 0.6625 - val_accuracy: 0.7028\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 584s 3s/step - loss: 0.5876 - accuracy: 0.7456 - val_loss: 0.5057 - val_accuracy: 0.7462\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 575s 3s/step - loss: 0.5802 - accuracy: 0.7500 - val_loss: 0.5581 - val_accuracy: 0.7625\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 574s 3s/step - loss: 0.5786 - accuracy: 0.7510 - val_loss: 0.5534 - val_accuracy: 0.7556\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 571s 3s/step - loss: 0.5670 - accuracy: 0.7547 - val_loss: 0.5681 - val_accuracy: 0.7637\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 565s 3s/step - loss: 0.5630 - accuracy: 0.7586 - val_loss: 0.4149 - val_accuracy: 0.7663\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 569s 3s/step - loss: 0.5501 - accuracy: 0.7656 - val_loss: 0.6044 - val_accuracy: 0.7809\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 500s 3s/step - loss: 0.5577 - accuracy: 0.7624 - val_loss: 0.4659 - val_accuracy: 0.7834\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    " \n",
    "#1st convolution layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48,48,3)))\n",
    "classifier.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\n",
    "classifier.add(BatchNormalization()) \n",
    "classifier.add(Dropout(0.2))\n",
    "#2nd convolution layer\n",
    "classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "classifier.add(BatchNormalization()) \n",
    "classifier.add(Dropout(0.2))\n",
    "#3rd convolution layer\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "classifier.add(BatchNormalization()) \n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dropout(0.2))\n",
    " \n",
    "#fully connected neural networks\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(units = 64, activation = 'relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(units =3, activation = 'softmax'))\n",
    "\n",
    "\n",
    " \n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('D:\\images/train',#folder names\n",
    "                                                 target_size = (48,48),\n",
    "                                                 batch_size = 90,  #16095/100\n",
    "                                                 class_mode = 'categorical')#train set    \n",
    "\n",
    "test_set = test_datagen.flow_from_directory('D:\\images/validation',\n",
    "                                            target_size = (48,48),\n",
    "                                            batch_size = 90, \n",
    "                                            class_mode = 'categorical')#test set\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         samples_per_epoch = 16095,#total  train set i have 351 happy,sad and angry train added\n",
    "                         nb_epoch =20,\n",
    "                         validation_data = test_set,\n",
    "                         nb_val_samples = 3924)#how many samples in test happy sad and anry added\n",
    "\n",
    "\n",
    "classifier.save('express.h5')\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
